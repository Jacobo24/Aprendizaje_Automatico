{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f64a029",
   "metadata": {},
   "source": [
    "# Proyecto BBVA y Santander"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fb59a",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fe9bafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd75d5",
   "metadata": {},
   "source": [
    "## DESCARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e43db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_15880\\876204869.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(t, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_15880\\876204869.py:7: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(t, start=start_date, end=end_date)\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "tickers = [\"BBVA.MC\", \"SAN.MC\"]  # BBVA y Banco Santander\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "\n",
    "data = {}\n",
    "for t in tickers:\n",
    "    df = yf.download(t, start=start_date, end=end_date)\n",
    "    data[t] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c6564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date         Close          High           Low          Open  Volume  \\\n",
      "0 2000-01-03  11609.988281  11881.787812  11574.388708  11641.388640       0   \n",
      "1 2000-01-04  11206.587891  11529.987943  11159.788135  11529.987943       0   \n",
      "2 2000-01-05  10863.088867  11068.088664  10824.889686  11068.088664       0   \n",
      "3 2000-01-07  11102.388672  11137.888634  10882.688708  10882.688708       0   \n",
      "4 2000-01-10  11173.288086  11364.287886  11120.587946  11120.587946       0   \n",
      "\n",
      "          Price  \n",
      "0  11609.988281  \n",
      "1  11206.587891  \n",
      "2  10863.088867  \n",
      "3  11102.388672  \n",
      "4  11173.288086  \n",
      "Date      datetime64[ns]\n",
      "Close            float64\n",
      "High             float64\n",
      "Low              float64\n",
      "Open             float64\n",
      "Volume             int64\n",
      "Price            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. leer normal (una sola cabecera)\n",
    "df = pd.read_csv(\"csv\\ibex35_data.csv\")\n",
    "\n",
    "# ahora df.head() será algo como:\n",
    "#    Price   Close    High     Low    Open  Volume\n",
    "# 0  Ticker  ^IBEX    ^IBEX   ... \n",
    "# 1  Date    NaN      NaN     ...\n",
    "# 2  2000-01-03 11609 ... \n",
    "\n",
    "# 2. eliminar las filas “raras” (las 2 primeras)\n",
    "df = df[~df[\"Price\"].isin([\"Ticker\", \"Date\"])].copy()\n",
    "\n",
    "# 3. renombrar la primera columna porque en realidad es la fecha\n",
    "df = df.rename(columns={\"Price\": \"Date\"})\n",
    "\n",
    "# 4. convertir la fecha\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# 5. convertir el resto a numérico\n",
    "for col in [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Para ser coherentes con lo anterior, creamos una columna PRICE fija:\n",
    "df[\"Price\"] = pd.to_numeric(df[\"Close\"], errors=\"coerce\")\n",
    "\n",
    "# ordenar por fecha\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740e14f",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c1cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ibex: (6072, 30, 1)\n",
      "y_ibex: (6072,)\n"
     ]
    }
   ],
   "source": [
    "def preparar_datos_desde_df(df, col_target=\"Price\", window_size=30):\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    if col_target not in df.columns:\n",
    "        raise ValueError(f\"No encuentro la columna {col_target}. Tengo: {df.columns.tolist()}\")\n",
    "\n",
    "    valores = df[[col_target]].values.astype(float)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    valores_scaled = scaler.fit_transform(valores)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(window_size, len(valores_scaled)):\n",
    "        X.append(valores_scaled[i-window_size:i, 0])\n",
    "        y.append(valores_scaled[i, 0])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return X, y, scaler, df\n",
    "\n",
    "window_size = 30\n",
    "X_ibex, y_ibex, scaler_ibex, ibex_df = preparar_datos_desde_df(df, col_target=\"Price\", window_size=window_size)\n",
    "\n",
    "print(\"X_ibex:\", X_ibex.shape)\n",
    "print(\"y_ibex:\", y_ibex.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98f193",
   "metadata": {},
   "source": [
    "## Train y test temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "357ddfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (4857, 30, 1) (4857,)\n",
      "Test : (1215, 30, 1) (1215,)\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 20% test (temporal, sin shuffle)\n",
    "train_size = int(len(X_ibex) * 0.8)\n",
    "X_train, X_test = X_ibex[:train_size], X_ibex[train_size:]\n",
    "y_train, y_test = y_ibex[:train_size], y_ibex[train_size:]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Test :\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b6d0d",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8be3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (window_size, 1)\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        SimpleRNN(64, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_gru():\n",
    "    model = Sequential([\n",
    "        GRU(64, input_shape=input_shape),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a439c",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d26dffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelo RNN...\n",
      "\n",
      "Epoch 1/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0089\n",
      "Epoch 1: val_loss improved from None to 0.00064, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0030 - val_loss: 6.3969e-04\n",
      "Epoch 2/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.6431e-04\n",
      "Epoch 2: val_loss improved from 0.00064 to 0.00040, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.9584e-04 - val_loss: 3.9964e-04\n",
      "Epoch 3/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0832e-04\n",
      "Epoch 3: val_loss improved from 0.00040 to 0.00032, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5.7370e-04 - val_loss: 3.2499e-04\n",
      "Epoch 4/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8505e-04\n",
      "Epoch 4: val_loss improved from 0.00032 to 0.00032, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.6446e-04 - val_loss: 3.2098e-04\n",
      "Epoch 5/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.0197e-04\n",
      "Epoch 5: val_loss improved from 0.00032 to 0.00027, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.8212e-04 - val_loss: 2.6638e-04\n",
      "Epoch 6/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.5680e-04\n",
      "Epoch 6: val_loss improved from 0.00027 to 0.00025, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 3.3496e-04 - val_loss: 2.4524e-04\n",
      "Epoch 7/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0704e-04\n",
      "Epoch 7: val_loss improved from 0.00025 to 0.00021, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1312e-04 - val_loss: 2.1227e-04\n",
      "Epoch 8/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9961e-04\n",
      "Epoch 8: val_loss improved from 0.00021 to 0.00015, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.7765e-04 - val_loss: 1.5153e-04\n",
      "Epoch 9/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6745e-04\n",
      "Epoch 9: val_loss did not improve from 0.00015\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.5999e-04 - val_loss: 1.5212e-04\n",
      "Epoch 10/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4873e-04\n",
      "Epoch 10: val_loss improved from 0.00015 to 0.00015, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.4660e-04 - val_loss: 1.4984e-04\n",
      "Epoch 11/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4811e-04\n",
      "Epoch 11: val_loss did not improve from 0.00015\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.5118e-04 - val_loss: 1.7670e-04\n",
      "Epoch 12/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0515e-04\n",
      "Epoch 12: val_loss did not improve from 0.00015\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.5107e-04 - val_loss: 1.7141e-04\n",
      "Epoch 13/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3203e-04\n",
      "Epoch 13: val_loss improved from 0.00015 to 0.00012, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1881e-04 - val_loss: 1.2337e-04\n",
      "Epoch 14/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1181e-04\n",
      "Epoch 14: val_loss improved from 0.00012 to 0.00012, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0328e-04 - val_loss: 1.1555e-04\n",
      "Epoch 15/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4048e-04\n",
      "Epoch 15: val_loss improved from 0.00012 to 0.00011, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2049e-04 - val_loss: 1.1027e-04\n",
      "Epoch 16/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9399e-04\n",
      "Epoch 16: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9382e-04 - val_loss: 1.1111e-04\n",
      "Epoch 17/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9839e-04\n",
      "Epoch 17: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9527e-04 - val_loss: 1.1919e-04\n",
      "Epoch 18/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9516e-04\n",
      "Epoch 18: val_loss improved from 0.00011 to 0.00011, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8967e-04 - val_loss: 1.0634e-04\n",
      "Epoch 19/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8897e-04\n",
      "Epoch 19: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8989e-04 - val_loss: 1.0836e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7946e-04\n",
      "Epoch 20: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9511e-04 - val_loss: 1.2001e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8186e-04\n",
      "Epoch 21: val_loss improved from 0.00011 to 0.00011, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8634e-04 - val_loss: 1.0538e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9746e-04\n",
      "Epoch 22: val_loss improved from 0.00011 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.9425e-04 - val_loss: 1.0294e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8235e-04\n",
      "Epoch 23: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8794e-04 - val_loss: 1.1268e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0359e-04\n",
      "Epoch 24: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9152e-04 - val_loss: 1.1468e-04\n",
      "Epoch 25/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9540e-04\n",
      "Epoch 25: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0300e-04 - val_loss: 1.0173e-04\n",
      "Epoch 26/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7718e-04\n",
      "Epoch 26: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8505e-04 - val_loss: 1.0140e-04\n",
      "Epoch 27/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7124e-04\n",
      "Epoch 27: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.8479e-04 - val_loss: 1.0123e-04\n",
      "Epoch 28/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8105e-04\n",
      "Epoch 28: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9229e-04 - val_loss: 1.3039e-04\n",
      "Epoch 29/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9282e-04\n",
      "Epoch 29: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8861e-04 - val_loss: 1.2621e-04\n",
      "Epoch 30/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8601e-04\n",
      "Epoch 30: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8578e-04 - val_loss: 1.0324e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9187e-04\n",
      "Epoch 31: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9484e-04 - val_loss: 1.1330e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8183e-04\n",
      "Epoch 32: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8520e-04 - val_loss: 1.0126e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6793e-04\n",
      "Epoch 33: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8582e-04 - val_loss: 1.0114e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7723e-04\n",
      "Epoch 34: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8681e-04 - val_loss: 1.0628e-04\n",
      "Epoch 35/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0462e-04\n",
      "Epoch 35: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8746e-04 - val_loss: 1.2795e-04\n",
      "Epoch 36/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6961e-04\n",
      "Epoch 36: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7889e-04 - val_loss: 1.1389e-04\n",
      "Epoch 37/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8378e-04\n",
      "Epoch 37: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8344e-04 - val_loss: 1.0225e-04\n",
      "Epoch 38/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7168e-04\n",
      "Epoch 38: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8528e-04 - val_loss: 1.0102e-04\n",
      "Epoch 39/1000\n",
      "\u001b[1m128/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7196e-04\n",
      "Epoch 39: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7541e-04 - val_loss: 1.0371e-04\n",
      "Epoch 40/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7599e-04\n",
      "Epoch 40: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8636e-04 - val_loss: 1.2016e-04\n",
      "Epoch 41/1000\n",
      "\u001b[1m134/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9167e-04\n",
      "Epoch 41: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7593e-04 - val_loss: 1.0466e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8206e-04\n",
      "Epoch 42: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8334e-04 - val_loss: 1.1907e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m129/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8495e-04\n",
      "Epoch 43: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8749e-04 - val_loss: 1.0187e-04\n",
      "Epoch 44/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8382e-04\n",
      "Epoch 44: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8446e-04 - val_loss: 1.0159e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9979e-04\n",
      "Epoch 45: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8454e-04 - val_loss: 1.0441e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6764e-04\n",
      "Epoch 46: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8504e-04 - val_loss: 1.0383e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5875e-04\n",
      "Epoch 47: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8337e-04 - val_loss: 1.2542e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1261e-04\n",
      "Epoch 48: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9309e-04 - val_loss: 1.0365e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7749e-04\n",
      "Epoch 49: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7916e-04 - val_loss: 1.0438e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9855e-04\n",
      "Epoch 50: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9221e-04 - val_loss: 1.0964e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7183e-04\n",
      "Epoch 51: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7754e-04 - val_loss: 1.1738e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6806e-04\n",
      "Epoch 52: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8333e-04 - val_loss: 1.0400e-04\n",
      "Epoch 53/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8095e-04\n",
      "Epoch 53: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8182e-04 - val_loss: 1.0188e-04\n",
      "Epoch 54/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8168e-04\n",
      "Epoch 54: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8687e-04 - val_loss: 1.2813e-04\n",
      "Epoch 55/1000\n",
      "\u001b[1m129/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8488e-04\n",
      "Epoch 55: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7616e-04 - val_loss: 1.1066e-04\n",
      "Epoch 56/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7512e-04\n",
      "Epoch 56: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8021e-04 - val_loss: 1.0771e-04\n",
      "Epoch 57/1000\n",
      "\u001b[1m129/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7218e-04\n",
      "Epoch 57: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8779e-04 - val_loss: 1.0267e-04\n",
      "Epoch 58/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7276e-04\n",
      "Epoch 58: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7951e-04 - val_loss: 1.1652e-04\n",
      "Epoch 59/1000\n",
      "\u001b[1m132/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9520e-04\n",
      "Epoch 59: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8412e-04 - val_loss: 1.0850e-04\n",
      "Epoch 60/1000\n",
      "\u001b[1m133/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8481e-04\n",
      "Epoch 60: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7856e-04 - val_loss: 1.0539e-04\n",
      "Epoch 61/1000\n",
      "\u001b[1m128/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8537e-04\n",
      "Epoch 61: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8258e-04 - val_loss: 1.0297e-04\n",
      "Epoch 62/1000\n",
      "\u001b[1m133/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6199e-04\n",
      "Epoch 62: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7957e-04 - val_loss: 1.7522e-04\n",
      "Epoch 63/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0545e-04\n",
      "Epoch 63: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8964e-04 - val_loss: 1.2644e-04\n",
      "Epoch 64/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7425e-04\n",
      "Epoch 64: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8740e-04 - val_loss: 1.0080e-04\n",
      "Epoch 65/1000\n",
      "\u001b[1m136/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7971e-04\n",
      "Epoch 65: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8661e-04 - val_loss: 1.0621e-04\n",
      "Epoch 66/1000\n",
      "\u001b[1m128/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8952e-04\n",
      "Epoch 66: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8149e-04 - val_loss: 1.0511e-04\n",
      "Epoch 67/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6578e-04\n",
      "Epoch 67: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7818e-04 - val_loss: 1.0264e-04\n",
      "Epoch 68/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7581e-04\n",
      "Epoch 68: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8450e-04 - val_loss: 1.1264e-04\n",
      "Epoch 69/1000\n",
      "\u001b[1m131/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8532e-04\n",
      "Epoch 69: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8104e-04 - val_loss: 1.0085e-04\n",
      "Epoch 70/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9718e-04\n",
      "Epoch 70: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9284e-04 - val_loss: 1.0176e-04\n",
      "Epoch 71/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7959e-04\n",
      "Epoch 71: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8027e-04 - val_loss: 1.0387e-04\n",
      "Epoch 72/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9590e-04\n",
      "Epoch 72: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8709e-04 - val_loss: 1.0662e-04\n",
      "Epoch 73/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7639e-04\n",
      "Epoch 73: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7933e-04 - val_loss: 1.0840e-04\n",
      "Epoch 74/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8797e-04\n",
      "Epoch 74: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7760e-04 - val_loss: 9.9756e-05\n",
      "Epoch 75/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7495e-04\n",
      "Epoch 75: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8995e-04 - val_loss: 1.0364e-04\n",
      "Epoch 76/1000\n",
      "\u001b[1m127/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8578e-04\n",
      "Epoch 76: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8948e-04 - val_loss: 1.0153e-04\n",
      "Epoch 77/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8340e-04\n",
      "Epoch 77: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8199e-04 - val_loss: 1.3863e-04\n",
      "Epoch 78/1000\n",
      "\u001b[1m128/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9191e-04\n",
      "Epoch 78: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8294e-04 - val_loss: 1.0537e-04\n",
      "Epoch 79/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7376e-04\n",
      "Epoch 79: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7580e-04 - val_loss: 1.0733e-04\n",
      "Epoch 80/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7624e-04\n",
      "Epoch 80: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8164e-04 - val_loss: 1.0070e-04\n",
      "Epoch 81/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7910e-04\n",
      "Epoch 81: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7858e-04 - val_loss: 1.0466e-04\n",
      "Epoch 82/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9053e-04\n",
      "Epoch 82: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8358e-04 - val_loss: 1.4824e-04\n",
      "Epoch 83/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9190e-04\n",
      "Epoch 83: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7910e-04 - val_loss: 1.0171e-04\n",
      "Epoch 84/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7630e-04\n",
      "Epoch 84: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8046e-04 - val_loss: 1.0465e-04\n",
      "Epoch 85/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7742e-04\n",
      "Epoch 85: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8742e-04 - val_loss: 1.0226e-04\n",
      "Epoch 86/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0638e-04\n",
      "Epoch 86: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8579e-04 - val_loss: 1.0617e-04\n",
      "Epoch 87/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7623e-04\n",
      "Epoch 87: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7835e-04 - val_loss: 1.2160e-04\n",
      "Epoch 88/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8065e-04\n",
      "Epoch 88: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8437e-04 - val_loss: 1.2678e-04\n",
      "Epoch 89/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8269e-04\n",
      "Epoch 89: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7929e-04 - val_loss: 1.0202e-04\n",
      "Epoch 90/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7460e-04\n",
      "Epoch 90: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7974e-04 - val_loss: 1.0582e-04\n",
      "Epoch 91/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8576e-04\n",
      "Epoch 91: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8030e-04 - val_loss: 1.1360e-04\n",
      "Epoch 92/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6978e-04\n",
      "Epoch 92: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8010e-04 - val_loss: 1.0450e-04\n",
      "Epoch 93/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0761e-04\n",
      "Epoch 93: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9207e-04 - val_loss: 1.0742e-04\n",
      "Epoch 94/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7446e-04\n",
      "Epoch 94: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8315e-04 - val_loss: 1.0357e-04\n",
      "Epoch 95/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9024e-04\n",
      "Epoch 95: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9201e-04 - val_loss: 1.2804e-04\n",
      "Epoch 96/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8680e-04\n",
      "Epoch 96: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8662e-04 - val_loss: 1.0602e-04\n",
      "Epoch 97/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8584e-04\n",
      "Epoch 97: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8178e-04 - val_loss: 1.1171e-04\n",
      "Epoch 98/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9090e-04\n",
      "Epoch 98: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8490e-04 - val_loss: 1.0043e-04\n",
      "Epoch 99/1000\n",
      "\u001b[1m136/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7250e-04\n",
      "Epoch 99: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8165e-04 - val_loss: 1.2015e-04\n",
      "Epoch 100/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7942e-04\n",
      "Epoch 100: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7738e-04 - val_loss: 1.0582e-04\n",
      "Epoch 101/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8998e-04\n",
      "Epoch 101: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8387e-04 - val_loss: 1.0085e-04\n",
      "Epoch 102/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9048e-04\n",
      "Epoch 102: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8257e-04 - val_loss: 1.0994e-04\n",
      "Epoch 103/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9587e-04\n",
      "Epoch 103: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8623e-04 - val_loss: 1.0333e-04\n",
      "Epoch 104/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9271e-04\n",
      "Epoch 104: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8192e-04 - val_loss: 1.0145e-04\n",
      "Epoch 105/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7318e-04\n",
      "Epoch 105: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8184e-04 - val_loss: 1.0085e-04\n",
      "Epoch 106/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6694e-04\n",
      "Epoch 106: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7686e-04 - val_loss: 1.0886e-04\n",
      "Epoch 107/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7443e-04\n",
      "Epoch 107: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8114e-04 - val_loss: 1.0395e-04\n",
      "Epoch 108/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7443e-04\n",
      "Epoch 108: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8950e-04 - val_loss: 1.1624e-04\n",
      "Epoch 109/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8476e-04\n",
      "Epoch 109: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7861e-04 - val_loss: 9.9269e-05\n",
      "Epoch 110/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8362e-04\n",
      "Epoch 110: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8594e-04 - val_loss: 1.0129e-04\n",
      "Epoch 111/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7804e-04\n",
      "Epoch 111: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8764e-04 - val_loss: 1.0554e-04\n",
      "Epoch 112/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8590e-04\n",
      "Epoch 112: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8019e-04 - val_loss: 1.0148e-04\n",
      "Epoch 113/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7137e-04\n",
      "Epoch 113: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8290e-04 - val_loss: 1.0070e-04\n",
      "Epoch 114/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7568e-04\n",
      "Epoch 114: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8123e-04 - val_loss: 1.0384e-04\n",
      "Epoch 115/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8208e-04\n",
      "Epoch 115: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8067e-04 - val_loss: 1.0433e-04\n",
      "Epoch 116/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7496e-04\n",
      "Epoch 116: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8170e-04 - val_loss: 1.1156e-04\n",
      "Epoch 117/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9104e-04\n",
      "Epoch 117: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8275e-04 - val_loss: 1.0586e-04\n",
      "Epoch 118/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6265e-04\n",
      "Epoch 118: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7812e-04 - val_loss: 1.0412e-04\n",
      "Epoch 119/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8960e-04\n",
      "Epoch 119: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8515e-04 - val_loss: 1.1071e-04\n",
      "Epoch 120/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8792e-04\n",
      "Epoch 120: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8297e-04 - val_loss: 1.0412e-04\n",
      "Epoch 121/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7142e-04\n",
      "Epoch 121: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7731e-04 - val_loss: 1.0234e-04\n",
      "Epoch 122/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8923e-04\n",
      "Epoch 122: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8265e-04 - val_loss: 1.1339e-04\n",
      "Epoch 123/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6344e-04\n",
      "Epoch 123: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8213e-04 - val_loss: 9.9239e-05\n",
      "Epoch 124/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6681e-04\n",
      "Epoch 124: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8062e-04 - val_loss: 1.0022e-04\n",
      "Epoch 125/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8347e-04\n",
      "Epoch 125: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7990e-04 - val_loss: 1.0206e-04\n",
      "Epoch 126/1000\n",
      "\u001b[1m133/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8159e-04\n",
      "Epoch 126: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8275e-04 - val_loss: 1.0218e-04\n",
      "Epoch 127/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7111e-04\n",
      "Epoch 127: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8210e-04 - val_loss: 1.0167e-04\n",
      "Epoch 128/1000\n",
      "\u001b[1m138/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7733e-04\n",
      "Epoch 128: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7702e-04 - val_loss: 1.0577e-04\n",
      "Epoch 129/1000\n",
      "\u001b[1m134/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8297e-04\n",
      "Epoch 129: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7860e-04 - val_loss: 1.0738e-04\n",
      "Epoch 130/1000\n",
      "\u001b[1m136/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8389e-04\n",
      "Epoch 130: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7907e-04 - val_loss: 1.0032e-04\n",
      "Epoch 131/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8875e-04\n",
      "Epoch 131: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8060e-04 - val_loss: 1.0845e-04\n",
      "Epoch 132/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8966e-04\n",
      "Epoch 132: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9083e-04 - val_loss: 1.0271e-04\n",
      "Epoch 133/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7486e-04\n",
      "Epoch 133: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7885e-04 - val_loss: 9.9201e-05\n",
      "Epoch 134/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9766e-04\n",
      "Epoch 134: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8463e-04 - val_loss: 1.1555e-04\n",
      "Epoch 135/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7666e-04\n",
      "Epoch 135: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8062e-04 - val_loss: 1.1431e-04\n",
      "Epoch 136/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9028e-04\n",
      "Epoch 136: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8266e-04 - val_loss: 1.1764e-04\n",
      "Epoch 137/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7967e-04\n",
      "Epoch 137: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7930e-04 - val_loss: 1.0315e-04\n",
      "Epoch 138/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8254e-04\n",
      "Epoch 138: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8140e-04 - val_loss: 1.0612e-04\n",
      "Epoch 139/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8034e-04\n",
      "Epoch 139: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7811e-04 - val_loss: 1.0376e-04\n",
      "Epoch 140/1000\n",
      "\u001b[1m127/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6944e-04\n",
      "Epoch 140: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7933e-04 - val_loss: 1.1986e-04\n",
      "Epoch 141/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8627e-04\n",
      "Epoch 141: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7879e-04 - val_loss: 1.0256e-04\n",
      "Epoch 142/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7442e-04\n",
      "Epoch 142: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8268e-04 - val_loss: 1.5869e-04\n",
      "Epoch 143/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8980e-04\n",
      "Epoch 143: val_loss improved from 0.00010 to 0.00010, saving model to mejor_rnn_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7705e-04 - val_loss: 9.8916e-05\n",
      "Epoch 144/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8399e-04\n",
      "Epoch 144: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8002e-04 - val_loss: 1.3510e-04\n",
      "Epoch 145/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9365e-04\n",
      "Epoch 145: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8273e-04 - val_loss: 1.0256e-04\n",
      "Epoch 146/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8406e-04\n",
      "Epoch 146: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7932e-04 - val_loss: 1.1136e-04\n",
      "Epoch 147/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9426e-04\n",
      "Epoch 147: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7933e-04 - val_loss: 1.0575e-04\n",
      "Epoch 148/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7885e-04\n",
      "Epoch 148: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7641e-04 - val_loss: 1.2192e-04\n",
      "Epoch 149/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6727e-04\n",
      "Epoch 149: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8441e-04 - val_loss: 1.0698e-04\n",
      "Epoch 150/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7860e-04\n",
      "Epoch 150: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8503e-04 - val_loss: 1.0547e-04\n",
      "Epoch 151/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7979e-04\n",
      "Epoch 151: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7697e-04 - val_loss: 1.0172e-04\n",
      "Epoch 152/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7329e-04\n",
      "Epoch 152: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8542e-04 - val_loss: 1.0175e-04\n",
      "Epoch 153/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7774e-04\n",
      "Epoch 153: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7931e-04 - val_loss: 9.9110e-05\n",
      "Epoch 154/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8739e-04\n",
      "Epoch 154: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7915e-04 - val_loss: 1.0145e-04\n",
      "Epoch 155/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7100e-04\n",
      "Epoch 155: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8221e-04 - val_loss: 1.0287e-04\n",
      "Epoch 156/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8599e-04\n",
      "Epoch 156: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7982e-04 - val_loss: 1.0457e-04\n",
      "Epoch 157/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7946e-04\n",
      "Epoch 157: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8021e-04 - val_loss: 1.0323e-04\n",
      "Epoch 158/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8269e-04\n",
      "Epoch 158: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8074e-04 - val_loss: 1.0946e-04\n",
      "Epoch 159/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8768e-04\n",
      "Epoch 159: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8062e-04 - val_loss: 1.2332e-04\n",
      "Epoch 160/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7652e-04\n",
      "Epoch 160: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8677e-04 - val_loss: 1.0951e-04\n",
      "Epoch 161/1000\n",
      "\u001b[1m138/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7619e-04\n",
      "Epoch 161: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7846e-04 - val_loss: 1.0107e-04\n",
      "Epoch 162/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8912e-04\n",
      "Epoch 162: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8183e-04 - val_loss: 1.0276e-04\n",
      "Epoch 163/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7874e-04\n",
      "Epoch 163: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8006e-04 - val_loss: 1.1038e-04\n",
      "Epoch 164/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9019e-04\n",
      "Epoch 164: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8090e-04 - val_loss: 9.9464e-05\n",
      "Epoch 165/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9480e-04\n",
      "Epoch 165: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8466e-04 - val_loss: 1.0048e-04\n",
      "Epoch 166/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7093e-04\n",
      "Epoch 166: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8169e-04 - val_loss: 1.0076e-04\n",
      "Epoch 167/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7759e-04\n",
      "Epoch 167: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8193e-04 - val_loss: 1.1201e-04\n",
      "Epoch 168/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7409e-04\n",
      "Epoch 168: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7971e-04 - val_loss: 1.0821e-04\n",
      "Epoch 169/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8119e-04\n",
      "Epoch 169: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7690e-04 - val_loss: 1.0665e-04\n",
      "Epoch 170/1000\n",
      "\u001b[1m136/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7707e-04\n",
      "Epoch 170: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8307e-04 - val_loss: 1.0139e-04\n",
      "Epoch 171/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7794e-04\n",
      "Epoch 171: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7746e-04 - val_loss: 1.0797e-04\n",
      "Epoch 172/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8253e-04\n",
      "Epoch 172: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8254e-04 - val_loss: 1.0430e-04\n",
      "Epoch 173/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7072e-04\n",
      "Epoch 173: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7670e-04 - val_loss: 1.0342e-04\n",
      "Epoch 174/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7145e-04\n",
      "Epoch 174: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8171e-04 - val_loss: 1.0437e-04\n",
      "Epoch 175/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8454e-04\n",
      "Epoch 175: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8005e-04 - val_loss: 9.9753e-05\n",
      "Epoch 176/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7121e-04\n",
      "Epoch 176: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7794e-04 - val_loss: 1.0120e-04\n",
      "Epoch 177/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7610e-04\n",
      "Epoch 177: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7712e-04 - val_loss: 1.0174e-04\n",
      "Epoch 178/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7971e-04\n",
      "Epoch 178: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8192e-04 - val_loss: 1.0980e-04\n",
      "Epoch 179/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7287e-04\n",
      "Epoch 179: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8052e-04 - val_loss: 1.0057e-04\n",
      "Epoch 180/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8640e-04\n",
      "Epoch 180: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8110e-04 - val_loss: 1.0471e-04\n",
      "Epoch 181/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7538e-04\n",
      "Epoch 181: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8206e-04 - val_loss: 1.0541e-04\n",
      "Epoch 182/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7430e-04\n",
      "Epoch 182: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8033e-04 - val_loss: 1.0352e-04\n",
      "Epoch 183/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8128e-04\n",
      "Epoch 183: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8566e-04 - val_loss: 1.1578e-04\n",
      "Epoch 184/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8544e-04\n",
      "Epoch 184: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8320e-04 - val_loss: 1.0003e-04\n",
      "Epoch 185/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8478e-04\n",
      "Epoch 185: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7569e-04 - val_loss: 1.0137e-04\n",
      "Epoch 186/1000\n",
      "\u001b[1m130/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7533e-04\n",
      "Epoch 186: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8614e-04 - val_loss: 1.3771e-04\n",
      "Epoch 187/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.5090e-04\n",
      "Epoch 187: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0976e-04 - val_loss: 1.4507e-04\n",
      "Epoch 188/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9280e-04\n",
      "Epoch 188: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8224e-04 - val_loss: 1.4116e-04\n",
      "Epoch 189/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7465e-04\n",
      "Epoch 189: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7921e-04 - val_loss: 1.0230e-04\n",
      "Epoch 190/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6808e-04\n",
      "Epoch 190: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7705e-04 - val_loss: 1.0208e-04\n",
      "Epoch 191/1000\n",
      "\u001b[1m129/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5827e-04\n",
      "Epoch 191: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7496e-04 - val_loss: 1.1346e-04\n",
      "Epoch 192/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7688e-04\n",
      "Epoch 192: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8292e-04 - val_loss: 1.1662e-04\n",
      "Epoch 193/1000\n",
      "\u001b[1m139/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7975e-04\n",
      "Epoch 193: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8159e-04 - val_loss: 1.0136e-04\n",
      "Epoch 194/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9039e-04\n",
      "Epoch 194: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7950e-04 - val_loss: 1.0379e-04\n",
      "Epoch 195/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6626e-04\n",
      "Epoch 195: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7557e-04 - val_loss: 1.0119e-04\n",
      "Epoch 196/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5655e-04\n",
      "Epoch 196: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7578e-04 - val_loss: 1.0097e-04\n",
      "Epoch 197/1000\n",
      "\u001b[1m127/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8524e-04\n",
      "Epoch 197: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7376e-04 - val_loss: 1.0362e-04\n",
      "Epoch 198/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7754e-04\n",
      "Epoch 198: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7879e-04 - val_loss: 1.0390e-04\n",
      "Epoch 199/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7992e-04\n",
      "Epoch 199: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7836e-04 - val_loss: 1.1283e-04\n",
      "Epoch 200/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6318e-04\n",
      "Epoch 200: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7642e-04 - val_loss: 1.1520e-04\n",
      "Epoch 201/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7512e-04\n",
      "Epoch 201: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7438e-04 - val_loss: 1.3001e-04\n",
      "Epoch 202/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8760e-04\n",
      "Epoch 202: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7963e-04 - val_loss: 1.0828e-04\n",
      "Epoch 203/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8705e-04\n",
      "Epoch 203: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8016e-04 - val_loss: 1.0222e-04\n",
      "Epoch 204/1000\n",
      "\u001b[1m127/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8316e-04\n",
      "Epoch 204: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7864e-04 - val_loss: 1.0229e-04\n",
      "Epoch 205/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8062e-04\n",
      "Epoch 205: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8297e-04 - val_loss: 1.0425e-04\n",
      "Epoch 206/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8953e-04\n",
      "Epoch 206: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7842e-04 - val_loss: 1.0500e-04\n",
      "Epoch 207/1000\n",
      "\u001b[1m135/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7708e-04\n",
      "Epoch 207: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7898e-04 - val_loss: 1.0085e-04\n",
      "Epoch 208/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8351e-04\n",
      "Epoch 208: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8223e-04 - val_loss: 1.0291e-04\n",
      "Epoch 209/1000\n",
      "\u001b[1m137/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7398e-04\n",
      "Epoch 209: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7769e-04 - val_loss: 1.0199e-04\n",
      "Epoch 210/1000\n",
      "\u001b[1m138/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8260e-04\n",
      "Epoch 210: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7707e-04 - val_loss: 1.0605e-04\n",
      "Epoch 211/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7009e-04\n",
      "Epoch 211: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8005e-04 - val_loss: 1.1340e-04\n",
      "Epoch 212/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8864e-04\n",
      "Epoch 212: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8144e-04 - val_loss: 1.1621e-04\n",
      "Epoch 213/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9507e-04\n",
      "Epoch 213: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8553e-04 - val_loss: 1.0834e-04\n",
      "Epoch 214/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7257e-04\n",
      "Epoch 214: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7734e-04 - val_loss: 1.0829e-04\n",
      "Epoch 215/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7701e-04\n",
      "Epoch 215: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7682e-04 - val_loss: 1.1048e-04\n",
      "Epoch 216/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6622e-04\n",
      "Epoch 216: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7548e-04 - val_loss: 1.0041e-04\n",
      "Epoch 217/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8755e-04\n",
      "Epoch 217: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8632e-04 - val_loss: 1.0441e-04\n",
      "Epoch 218/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7908e-04\n",
      "Epoch 218: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8282e-04 - val_loss: 1.1187e-04\n",
      "Epoch 219/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7122e-04\n",
      "Epoch 219: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7687e-04 - val_loss: 1.0320e-04\n",
      "Epoch 220/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7215e-04\n",
      "Epoch 220: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7806e-04 - val_loss: 1.2090e-04\n",
      "Epoch 221/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7528e-04\n",
      "Epoch 221: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7906e-04 - val_loss: 1.0152e-04\n",
      "Epoch 222/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.8071e-04\n",
      "Epoch 222: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7566e-04 - val_loss: 1.0919e-04\n",
      "Epoch 223/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7586e-04\n",
      "Epoch 223: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7806e-04 - val_loss: 1.2574e-04\n",
      "\n",
      "Entrenando modelo LSTM...\n",
      "\n",
      "Epoch 1/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0162\n",
      "Epoch 1: val_loss improved from None to 0.00056, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0048 - val_loss: 5.6245e-04\n",
      "Epoch 2/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.5578e-04\n",
      "Epoch 2: val_loss improved from 0.00056 to 0.00050, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.4813e-04 - val_loss: 4.9515e-04\n",
      "Epoch 3/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.0381e-04\n",
      "Epoch 3: val_loss improved from 0.00050 to 0.00045, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5.7744e-04 - val_loss: 4.4722e-04\n",
      "Epoch 4/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5.3511e-04\n",
      "Epoch 4: val_loss improved from 0.00045 to 0.00039, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 5.2406e-04 - val_loss: 3.9126e-04\n",
      "Epoch 5/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9432e-04\n",
      "Epoch 5: val_loss improved from 0.00039 to 0.00036, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.8314e-04 - val_loss: 3.5845e-04\n",
      "Epoch 6/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.7153e-04\n",
      "Epoch 6: val_loss improved from 0.00036 to 0.00032, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.5971e-04 - val_loss: 3.1798e-04\n",
      "Epoch 7/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.4535e-04\n",
      "Epoch 7: val_loss improved from 0.00032 to 0.00029, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.1933e-04 - val_loss: 2.8906e-04\n",
      "Epoch 8/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9444e-04\n",
      "Epoch 8: val_loss improved from 0.00029 to 0.00027, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.1689e-04 - val_loss: 2.7228e-04\n",
      "Epoch 9/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.9056e-04\n",
      "Epoch 9: val_loss did not improve from 0.00027\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.8079e-04 - val_loss: 3.0983e-04\n",
      "Epoch 10/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.6248e-04\n",
      "Epoch 10: val_loss improved from 0.00027 to 0.00023, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.5883e-04 - val_loss: 2.3138e-04\n",
      "Epoch 11/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.4163e-04\n",
      "Epoch 11: val_loss improved from 0.00023 to 0.00023, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.4004e-04 - val_loss: 2.2717e-04\n",
      "Epoch 12/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2399e-04\n",
      "Epoch 12: val_loss improved from 0.00023 to 0.00021, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1201e-04 - val_loss: 2.0636e-04\n",
      "Epoch 13/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8385e-04\n",
      "Epoch 13: val_loss improved from 0.00021 to 0.00020, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.0007e-04 - val_loss: 1.9944e-04\n",
      "Epoch 14/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8273e-04\n",
      "Epoch 14: val_loss improved from 0.00020 to 0.00018, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.9047e-04 - val_loss: 1.8455e-04\n",
      "Epoch 15/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7359e-04\n",
      "Epoch 15: val_loss improved from 0.00018 to 0.00017, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.6789e-04 - val_loss: 1.6957e-04\n",
      "Epoch 16/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6769e-04\n",
      "Epoch 16: val_loss did not improve from 0.00017\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.6611e-04 - val_loss: 1.8722e-04\n",
      "Epoch 17/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5497e-04\n",
      "Epoch 17: val_loss improved from 0.00017 to 0.00015, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.5536e-04 - val_loss: 1.5300e-04\n",
      "Epoch 18/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.3422e-04\n",
      "Epoch 18: val_loss did not improve from 0.00015\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.3540e-04 - val_loss: 1.6044e-04\n",
      "Epoch 19/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5572e-04\n",
      "Epoch 19: val_loss improved from 0.00015 to 0.00014, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.4192e-04 - val_loss: 1.3970e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1458e-04\n",
      "Epoch 20: val_loss did not improve from 0.00014\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1761e-04 - val_loss: 1.4031e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1700e-04\n",
      "Epoch 21: val_loss did not improve from 0.00014\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.1560e-04 - val_loss: 1.7172e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.4333e-04\n",
      "Epoch 22: val_loss improved from 0.00014 to 0.00014, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2404e-04 - val_loss: 1.3967e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1742e-04\n",
      "Epoch 23: val_loss improved from 0.00014 to 0.00012, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1709e-04 - val_loss: 1.2313e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9229e-04\n",
      "Epoch 24: val_loss improved from 0.00012 to 0.00012, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9792e-04 - val_loss: 1.1730e-04\n",
      "Epoch 25/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9523e-04\n",
      "Epoch 25: val_loss improved from 0.00012 to 0.00011, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9193e-04 - val_loss: 1.1451e-04\n",
      "Epoch 26/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8965e-04\n",
      "Epoch 26: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0191e-04 - val_loss: 1.6259e-04\n",
      "Epoch 27/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9492e-04\n",
      "Epoch 27: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9592e-04 - val_loss: 1.1925e-04\n",
      "Epoch 28/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9594e-04\n",
      "Epoch 28: val_loss improved from 0.00011 to 0.00011, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9271e-04 - val_loss: 1.0671e-04\n",
      "Epoch 29/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8541e-04\n",
      "Epoch 29: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8420e-04 - val_loss: 1.0697e-04\n",
      "Epoch 30/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9162e-04\n",
      "Epoch 30: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8547e-04 - val_loss: 1.2293e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8141e-04\n",
      "Epoch 31: val_loss improved from 0.00011 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8288e-04 - val_loss: 1.0395e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8545e-04\n",
      "Epoch 32: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8274e-04 - val_loss: 1.0934e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7292e-04\n",
      "Epoch 33: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8018e-04 - val_loss: 1.1098e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9059e-04\n",
      "Epoch 34: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8795e-04 - val_loss: 1.0251e-04\n",
      "Epoch 35/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7687e-04\n",
      "Epoch 35: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8060e-04 - val_loss: 1.0259e-04\n",
      "Epoch 36/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7502e-04\n",
      "Epoch 36: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7838e-04 - val_loss: 1.0500e-04\n",
      "Epoch 37/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8390e-04\n",
      "Epoch 37: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8519e-04 - val_loss: 1.0096e-04\n",
      "Epoch 38/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7574e-04\n",
      "Epoch 38: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8216e-04 - val_loss: 1.0147e-04\n",
      "Epoch 39/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7649e-04\n",
      "Epoch 39: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7859e-04 - val_loss: 1.0245e-04\n",
      "Epoch 40/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9250e-04\n",
      "Epoch 40: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8557e-04 - val_loss: 1.0713e-04\n",
      "Epoch 41/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8251e-04\n",
      "Epoch 41: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8328e-04 - val_loss: 1.1357e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8946e-04\n",
      "Epoch 42: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8441e-04 - val_loss: 1.0107e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7113e-04\n",
      "Epoch 43: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7637e-04 - val_loss: 1.0057e-04\n",
      "Epoch 44/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8584e-04\n",
      "Epoch 44: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8144e-04 - val_loss: 1.0543e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7687e-04\n",
      "Epoch 45: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7728e-04 - val_loss: 1.1035e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.9326e-04\n",
      "Epoch 46: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8145e-04 - val_loss: 1.0540e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7586e-04\n",
      "Epoch 47: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7685e-04 - val_loss: 1.0396e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6710e-04\n",
      "Epoch 48: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7343e-04 - val_loss: 1.0785e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8063e-04\n",
      "Epoch 49: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7936e-04 - val_loss: 1.0667e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7691e-04\n",
      "Epoch 50: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7950e-04 - val_loss: 1.0312e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6593e-04\n",
      "Epoch 51: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7395e-04 - val_loss: 1.1518e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9228e-04\n",
      "Epoch 52: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7794e-04 - val_loss: 9.9663e-05\n",
      "Epoch 53/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7197e-04\n",
      "Epoch 53: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8018e-04 - val_loss: 1.0812e-04\n",
      "Epoch 54/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7932e-04\n",
      "Epoch 54: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7741e-04 - val_loss: 9.9983e-05\n",
      "Epoch 55/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7389e-04\n",
      "Epoch 55: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8211e-04 - val_loss: 1.1033e-04\n",
      "Epoch 56/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7540e-04\n",
      "Epoch 56: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7353e-04 - val_loss: 1.0619e-04\n",
      "Epoch 57/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7740e-04\n",
      "Epoch 57: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8013e-04 - val_loss: 9.9798e-05\n",
      "Epoch 58/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8845e-04\n",
      "Epoch 58: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8242e-04 - val_loss: 1.2876e-04\n",
      "Epoch 59/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8763e-04\n",
      "Epoch 59: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7826e-04 - val_loss: 9.9757e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6738e-04\n",
      "Epoch 60: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7442e-04 - val_loss: 1.0328e-04\n",
      "Epoch 61/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8199e-04\n",
      "Epoch 61: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7926e-04 - val_loss: 1.1003e-04\n",
      "Epoch 62/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7902e-04\n",
      "Epoch 62: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8183e-04 - val_loss: 9.9175e-05\n",
      "Epoch 63/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6258e-04\n",
      "Epoch 63: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7745e-04 - val_loss: 1.0952e-04\n",
      "Epoch 64/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7463e-04\n",
      "Epoch 64: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7545e-04 - val_loss: 1.1348e-04\n",
      "Epoch 65/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7708e-04\n",
      "Epoch 65: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7512e-04 - val_loss: 9.9517e-05\n",
      "Epoch 66/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7234e-04\n",
      "Epoch 66: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7775e-04 - val_loss: 1.0599e-04\n",
      "Epoch 67/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9285e-04\n",
      "Epoch 67: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7936e-04 - val_loss: 1.2418e-04\n",
      "Epoch 68/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7325e-04\n",
      "Epoch 68: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7880e-04 - val_loss: 1.1599e-04\n",
      "Epoch 69/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6673e-04\n",
      "Epoch 69: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7957e-04 - val_loss: 1.0610e-04\n",
      "Epoch 70/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9119e-04\n",
      "Epoch 70: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7579e-04 - val_loss: 1.0010e-04\n",
      "Epoch 71/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0528e-04\n",
      "Epoch 71: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8024e-04 - val_loss: 1.1549e-04\n",
      "Epoch 72/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.7531e-04\n",
      "Epoch 72: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7366e-04 - val_loss: 1.1000e-04\n",
      "Epoch 73/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8112e-04\n",
      "Epoch 73: val_loss improved from 0.00010 to 0.00010, saving model to mejor_lstm_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7922e-04 - val_loss: 9.9088e-05\n",
      "Epoch 74/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8060e-04\n",
      "Epoch 74: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7320e-04 - val_loss: 1.0020e-04\n",
      "Epoch 75/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6132e-04\n",
      "Epoch 75: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7687e-04 - val_loss: 1.7506e-04\n",
      "Epoch 76/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6463e-04\n",
      "Epoch 76: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7969e-04 - val_loss: 1.1054e-04\n",
      "Epoch 77/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8065e-04\n",
      "Epoch 77: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7536e-04 - val_loss: 9.9580e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6575e-04\n",
      "Epoch 78: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7801e-04 - val_loss: 9.9639e-05\n",
      "Epoch 79/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8097e-04\n",
      "Epoch 79: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7884e-04 - val_loss: 1.1707e-04\n",
      "Epoch 80/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8353e-04\n",
      "Epoch 80: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7716e-04 - val_loss: 9.9172e-05\n",
      "\n",
      "Entrenando modelo GRU...\n",
      "\n",
      "Epoch 1/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0278\n",
      "Epoch 1: val_loss improved from None to 0.00024, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0079 - val_loss: 2.4101e-04\n",
      "Epoch 2/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2014e-04\n",
      "Epoch 2: val_loss improved from 0.00024 to 0.00020, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.1629e-04 - val_loss: 2.0456e-04\n",
      "Epoch 3/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9811e-04\n",
      "Epoch 3: val_loss improved from 0.00020 to 0.00018, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.8278e-04 - val_loss: 1.8316e-04\n",
      "Epoch 4/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6410e-04\n",
      "Epoch 4: val_loss improved from 0.00018 to 0.00017, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.6228e-04 - val_loss: 1.6798e-04\n",
      "Epoch 5/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5510e-04\n",
      "Epoch 5: val_loss did not improve from 0.00017\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.4607e-04 - val_loss: 1.7476e-04\n",
      "Epoch 6/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4127e-04\n",
      "Epoch 6: val_loss improved from 0.00017 to 0.00015, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3650e-04 - val_loss: 1.4662e-04\n",
      "Epoch 7/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2758e-04\n",
      "Epoch 7: val_loss did not improve from 0.00015\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.3485e-04 - val_loss: 1.5675e-04\n",
      "Epoch 8/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2773e-04\n",
      "Epoch 8: val_loss improved from 0.00015 to 0.00014, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2570e-04 - val_loss: 1.4381e-04\n",
      "Epoch 9/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0896e-04\n",
      "Epoch 9: val_loss did not improve from 0.00014\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2405e-04 - val_loss: 1.5345e-04\n",
      "Epoch 10/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2930e-04\n",
      "Epoch 10: val_loss improved from 0.00014 to 0.00013, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.2047e-04 - val_loss: 1.2651e-04\n",
      "Epoch 11/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0274e-04\n",
      "Epoch 11: val_loss improved from 0.00013 to 0.00012, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1242e-04 - val_loss: 1.2475e-04\n",
      "Epoch 12/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0594e-04\n",
      "Epoch 12: val_loss improved from 0.00012 to 0.00012, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1006e-04 - val_loss: 1.2040e-04\n",
      "Epoch 13/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9385e-04\n",
      "Epoch 13: val_loss did not improve from 0.00012\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0040e-04 - val_loss: 1.2221e-04\n",
      "Epoch 14/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9767e-04\n",
      "Epoch 14: val_loss improved from 0.00012 to 0.00012, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9918e-04 - val_loss: 1.1617e-04\n",
      "Epoch 15/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9743e-04\n",
      "Epoch 15: val_loss did not improve from 0.00012\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0864e-04 - val_loss: 1.2271e-04\n",
      "Epoch 16/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9113e-04\n",
      "Epoch 16: val_loss did not improve from 0.00012\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9839e-04 - val_loss: 1.3166e-04\n",
      "Epoch 17/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.1057e-04\n",
      "Epoch 17: val_loss improved from 0.00012 to 0.00011, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0588e-04 - val_loss: 1.1122e-04\n",
      "Epoch 18/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.2637e-04\n",
      "Epoch 18: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1092e-04 - val_loss: 1.1507e-04\n",
      "Epoch 19/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9915e-04\n",
      "Epoch 19: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9452e-04 - val_loss: 1.2048e-04\n",
      "Epoch 20/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8696e-04\n",
      "Epoch 20: val_loss improved from 0.00011 to 0.00011, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8721e-04 - val_loss: 1.0820e-04\n",
      "Epoch 21/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7715e-04\n",
      "Epoch 21: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9127e-04 - val_loss: 1.2069e-04\n",
      "Epoch 22/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8615e-04\n",
      "Epoch 22: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9102e-04 - val_loss: 1.3516e-04\n",
      "Epoch 23/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0100e-04\n",
      "Epoch 23: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8661e-04 - val_loss: 1.0943e-04\n",
      "Epoch 24/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8320e-04\n",
      "Epoch 24: val_loss did not improve from 0.00011\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8642e-04 - val_loss: 1.1616e-04\n",
      "Epoch 25/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8494e-04\n",
      "Epoch 25: val_loss improved from 0.00011 to 0.00010, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8768e-04 - val_loss: 1.0268e-04\n",
      "Epoch 26/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8315e-04\n",
      "Epoch 26: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8266e-04 - val_loss: 1.1565e-04\n",
      "Epoch 27/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9134e-04\n",
      "Epoch 27: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8734e-04 - val_loss: 1.0813e-04\n",
      "Epoch 28/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8056e-04\n",
      "Epoch 28: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8444e-04 - val_loss: 1.1797e-04\n",
      "Epoch 29/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8851e-04\n",
      "Epoch 29: val_loss improved from 0.00010 to 0.00010, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9788e-04 - val_loss: 1.0129e-04\n",
      "Epoch 30/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9456e-04\n",
      "Epoch 30: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8394e-04 - val_loss: 1.0180e-04\n",
      "Epoch 31/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7806e-04\n",
      "Epoch 31: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8537e-04 - val_loss: 1.1116e-04\n",
      "Epoch 32/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9437e-04\n",
      "Epoch 32: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9297e-04 - val_loss: 1.2889e-04\n",
      "Epoch 33/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0238e-04\n",
      "Epoch 33: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9530e-04 - val_loss: 1.4393e-04\n",
      "Epoch 34/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8761e-04\n",
      "Epoch 34: val_loss improved from 0.00010 to 0.00010, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8604e-04 - val_loss: 1.0019e-04\n",
      "Epoch 35/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8512e-04\n",
      "Epoch 35: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8023e-04 - val_loss: 1.0182e-04\n",
      "Epoch 36/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7615e-04\n",
      "Epoch 36: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8187e-04 - val_loss: 1.1583e-04\n",
      "Epoch 37/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7108e-04\n",
      "Epoch 37: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7967e-04 - val_loss: 1.1902e-04\n",
      "Epoch 38/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7699e-04\n",
      "Epoch 38: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7846e-04 - val_loss: 1.0153e-04\n",
      "Epoch 39/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8215e-04\n",
      "Epoch 39: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8258e-04 - val_loss: 1.0544e-04\n",
      "Epoch 40/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8467e-04\n",
      "Epoch 40: val_loss improved from 0.00010 to 0.00010, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8860e-04 - val_loss: 9.9026e-05\n",
      "Epoch 41/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7705e-04\n",
      "Epoch 41: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8417e-04 - val_loss: 1.2778e-04\n",
      "Epoch 42/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8608e-04\n",
      "Epoch 42: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8011e-04 - val_loss: 1.0183e-04\n",
      "Epoch 43/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6710e-04\n",
      "Epoch 43: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7576e-04 - val_loss: 9.9400e-05\n",
      "Epoch 44/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8206e-04\n",
      "Epoch 44: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8883e-04 - val_loss: 1.1162e-04\n",
      "Epoch 45/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6216e-04\n",
      "Epoch 45: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7577e-04 - val_loss: 1.0053e-04\n",
      "Epoch 46/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8578e-04\n",
      "Epoch 46: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7654e-04 - val_loss: 1.2192e-04\n",
      "Epoch 47/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7926e-04\n",
      "Epoch 47: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7958e-04 - val_loss: 1.0047e-04\n",
      "Epoch 48/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6905e-04\n",
      "Epoch 48: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7691e-04 - val_loss: 1.0455e-04\n",
      "Epoch 49/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8106e-04\n",
      "Epoch 49: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8278e-04 - val_loss: 1.0335e-04\n",
      "Epoch 50/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.0116e-04\n",
      "Epoch 50: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8613e-04 - val_loss: 1.0097e-04\n",
      "Epoch 51/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7788e-04\n",
      "Epoch 51: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8204e-04 - val_loss: 1.0593e-04\n",
      "Epoch 52/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8241e-04\n",
      "Epoch 52: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8098e-04 - val_loss: 1.1149e-04\n",
      "Epoch 53/1000\n",
      "\u001b[1m141/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.5916e-04\n",
      "Epoch 53: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7347e-04 - val_loss: 1.1617e-04\n",
      "Epoch 54/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6948e-04\n",
      "Epoch 54: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7671e-04 - val_loss: 1.1024e-04\n",
      "Epoch 55/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8994e-04\n",
      "Epoch 55: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8214e-04 - val_loss: 9.9153e-05\n",
      "Epoch 56/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8777e-04\n",
      "Epoch 56: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9128e-04 - val_loss: 1.2232e-04\n",
      "Epoch 57/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8312e-04\n",
      "Epoch 57: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7618e-04 - val_loss: 1.0101e-04\n",
      "Epoch 58/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8671e-04\n",
      "Epoch 58: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8557e-04 - val_loss: 1.3818e-04\n",
      "Epoch 59/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8354e-04\n",
      "Epoch 59: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.9189e-04 - val_loss: 9.9457e-05\n",
      "Epoch 60/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7956e-04\n",
      "Epoch 60: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7903e-04 - val_loss: 9.9261e-05\n",
      "Epoch 61/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9731e-04\n",
      "Epoch 61: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8977e-04 - val_loss: 1.2678e-04\n",
      "Epoch 62/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8185e-04\n",
      "Epoch 62: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7499e-04 - val_loss: 1.0798e-04\n",
      "Epoch 63/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9415e-04\n",
      "Epoch 63: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8175e-04 - val_loss: 1.0480e-04\n",
      "Epoch 64/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9039e-04\n",
      "Epoch 64: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8485e-04 - val_loss: 1.1712e-04\n",
      "Epoch 65/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7735e-04\n",
      "Epoch 65: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8215e-04 - val_loss: 1.0894e-04\n",
      "Epoch 66/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7102e-04\n",
      "Epoch 66: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8448e-04 - val_loss: 1.1133e-04\n",
      "Epoch 67/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9428e-04\n",
      "Epoch 67: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7717e-04 - val_loss: 9.9497e-05\n",
      "Epoch 68/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9186e-04\n",
      "Epoch 68: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8201e-04 - val_loss: 1.0419e-04\n",
      "Epoch 69/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7340e-04\n",
      "Epoch 69: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7707e-04 - val_loss: 9.9448e-05\n",
      "Epoch 70/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8811e-04\n",
      "Epoch 70: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7630e-04 - val_loss: 1.0102e-04\n",
      "Epoch 71/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8576e-04\n",
      "Epoch 71: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8020e-04 - val_loss: 1.4720e-04\n",
      "Epoch 72/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7302e-04\n",
      "Epoch 72: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8226e-04 - val_loss: 9.9247e-05\n",
      "Epoch 73/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7720e-04\n",
      "Epoch 73: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7979e-04 - val_loss: 9.9482e-05\n",
      "Epoch 74/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9777e-04\n",
      "Epoch 74: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8599e-04 - val_loss: 9.9063e-05\n",
      "Epoch 75/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7802e-04\n",
      "Epoch 75: val_loss improved from 0.00010 to 0.00010, saving model to mejor_gru_ibex.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7667e-04 - val_loss: 9.8678e-05\n",
      "Epoch 76/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9506e-04\n",
      "Epoch 76: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8502e-04 - val_loss: 9.8807e-05\n",
      "Epoch 77/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8865e-04\n",
      "Epoch 77: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8278e-04 - val_loss: 9.9557e-05\n",
      "Epoch 78/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7575e-04\n",
      "Epoch 78: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7549e-04 - val_loss: 1.0640e-04\n",
      "Epoch 79/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8379e-04\n",
      "Epoch 79: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.8030e-04 - val_loss: 1.2794e-04\n",
      "Epoch 80/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8915e-04\n",
      "Epoch 80: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8240e-04 - val_loss: 1.0395e-04\n",
      "Epoch 81/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9152e-04\n",
      "Epoch 81: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8119e-04 - val_loss: 9.9931e-05\n",
      "Epoch 82/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8070e-04\n",
      "Epoch 82: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7402e-04 - val_loss: 9.8742e-05\n",
      "Epoch 83/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9114e-04\n",
      "Epoch 83: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7953e-04 - val_loss: 2.1336e-04\n",
      "Epoch 84/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9675e-04\n",
      "Epoch 84: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8204e-04 - val_loss: 1.3630e-04\n",
      "Epoch 85/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7860e-04\n",
      "Epoch 85: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7587e-04 - val_loss: 9.9200e-05\n",
      "Epoch 86/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8352e-04\n",
      "Epoch 86: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7545e-04 - val_loss: 9.9713e-05\n",
      "Epoch 87/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7439e-04\n",
      "Epoch 87: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8225e-04 - val_loss: 1.1923e-04\n",
      "Epoch 88/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8287e-04\n",
      "Epoch 88: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8125e-04 - val_loss: 9.9930e-05\n",
      "Epoch 89/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8259e-04\n",
      "Epoch 89: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7554e-04 - val_loss: 9.9706e-05\n",
      "Epoch 90/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7694e-04\n",
      "Epoch 90: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7706e-04 - val_loss: 1.1205e-04\n",
      "Epoch 91/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7261e-04\n",
      "Epoch 91: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7475e-04 - val_loss: 1.0180e-04\n",
      "Epoch 92/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8097e-04\n",
      "Epoch 92: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7679e-04 - val_loss: 9.8962e-05\n",
      "Epoch 93/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7260e-04\n",
      "Epoch 93: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8085e-04 - val_loss: 1.0040e-04\n",
      "Epoch 94/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7966e-04\n",
      "Epoch 94: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7532e-04 - val_loss: 1.0043e-04\n",
      "Epoch 95/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7174e-04\n",
      "Epoch 95: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7833e-04 - val_loss: 1.0017e-04\n",
      "Epoch 96/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6578e-04\n",
      "Epoch 96: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7875e-04 - val_loss: 1.1004e-04\n",
      "Epoch 97/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7908e-04\n",
      "Epoch 97: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7689e-04 - val_loss: 1.0270e-04\n",
      "Epoch 98/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9348e-04\n",
      "Epoch 98: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8420e-04 - val_loss: 1.0037e-04\n",
      "Epoch 99/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7817e-04\n",
      "Epoch 99: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7211e-04 - val_loss: 1.0642e-04\n",
      "Epoch 100/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6993e-04\n",
      "Epoch 100: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7712e-04 - val_loss: 1.0034e-04\n",
      "Epoch 101/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7449e-04\n",
      "Epoch 101: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7785e-04 - val_loss: 1.0461e-04\n",
      "Epoch 102/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8113e-04\n",
      "Epoch 102: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7712e-04 - val_loss: 1.1002e-04\n",
      "Epoch 103/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9685e-04\n",
      "Epoch 103: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7866e-04 - val_loss: 1.3017e-04\n",
      "Epoch 104/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8125e-04\n",
      "Epoch 104: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8173e-04 - val_loss: 1.0105e-04\n",
      "Epoch 105/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7514e-04\n",
      "Epoch 105: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8029e-04 - val_loss: 1.2496e-04\n",
      "Epoch 106/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9377e-04\n",
      "Epoch 106: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7659e-04 - val_loss: 1.0084e-04\n",
      "Epoch 107/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7059e-04\n",
      "Epoch 107: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7982e-04 - val_loss: 9.9450e-05\n",
      "Epoch 108/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6562e-04\n",
      "Epoch 108: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7925e-04 - val_loss: 1.0940e-04\n",
      "Epoch 109/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6905e-04\n",
      "Epoch 109: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7858e-04 - val_loss: 1.0042e-04\n",
      "Epoch 110/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6589e-04\n",
      "Epoch 110: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7599e-04 - val_loss: 1.0178e-04\n",
      "Epoch 111/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7275e-04\n",
      "Epoch 111: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7426e-04 - val_loss: 1.3766e-04\n",
      "Epoch 112/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8217e-04\n",
      "Epoch 112: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7460e-04 - val_loss: 1.0651e-04\n",
      "Epoch 113/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7562e-04\n",
      "Epoch 113: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7673e-04 - val_loss: 9.9887e-05\n",
      "Epoch 114/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7984e-04\n",
      "Epoch 114: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7916e-04 - val_loss: 1.0727e-04\n",
      "Epoch 115/1000\n",
      "\u001b[1m146/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7572e-04\n",
      "Epoch 115: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7636e-04 - val_loss: 1.1054e-04\n",
      "Epoch 116/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7214e-04\n",
      "Epoch 116: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7283e-04 - val_loss: 1.0440e-04\n",
      "Epoch 117/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7110e-04\n",
      "Epoch 117: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7847e-04 - val_loss: 9.9766e-05\n",
      "Epoch 118/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8280e-04\n",
      "Epoch 118: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8089e-04 - val_loss: 1.1826e-04\n",
      "Epoch 119/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7858e-04\n",
      "Epoch 119: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7505e-04 - val_loss: 1.0099e-04\n",
      "Epoch 120/1000\n",
      "\u001b[1m149/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7534e-04\n",
      "Epoch 120: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7763e-04 - val_loss: 1.0037e-04\n",
      "Epoch 121/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7040e-04\n",
      "Epoch 121: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.7586e-04 - val_loss: 1.0646e-04\n",
      "Epoch 122/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1.7542e-04\n",
      "Epoch 122: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7989e-04 - val_loss: 1.0564e-04\n",
      "Epoch 123/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6620e-04\n",
      "Epoch 123: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7825e-04 - val_loss: 1.0880e-04\n",
      "Epoch 124/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9785e-04\n",
      "Epoch 124: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7412e-04 - val_loss: 9.9614e-05\n",
      "Epoch 125/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6188e-04\n",
      "Epoch 125: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7194e-04 - val_loss: 1.1470e-04\n",
      "Epoch 126/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7367e-04\n",
      "Epoch 126: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7946e-04 - val_loss: 1.2409e-04\n",
      "Epoch 127/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.9669e-04\n",
      "Epoch 127: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7680e-04 - val_loss: 1.0454e-04\n",
      "Epoch 128/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7503e-04\n",
      "Epoch 128: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7944e-04 - val_loss: 1.0628e-04\n",
      "Epoch 129/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6801e-04\n",
      "Epoch 129: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7395e-04 - val_loss: 1.0387e-04\n",
      "Epoch 130/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8569e-04\n",
      "Epoch 130: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8005e-04 - val_loss: 1.0041e-04\n",
      "Epoch 131/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6305e-04\n",
      "Epoch 131: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7538e-04 - val_loss: 1.1142e-04\n",
      "Epoch 132/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8317e-04\n",
      "Epoch 132: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7562e-04 - val_loss: 1.0386e-04\n",
      "Epoch 133/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8508e-04\n",
      "Epoch 133: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8134e-04 - val_loss: 1.1019e-04\n",
      "Epoch 134/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8611e-04\n",
      "Epoch 134: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7227e-04 - val_loss: 1.0001e-04\n",
      "Epoch 135/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8078e-04\n",
      "Epoch 135: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7928e-04 - val_loss: 1.0401e-04\n",
      "Epoch 136/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6997e-04\n",
      "Epoch 136: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7408e-04 - val_loss: 1.0037e-04\n",
      "Epoch 137/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7407e-04\n",
      "Epoch 137: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7135e-04 - val_loss: 1.0415e-04\n",
      "Epoch 138/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7043e-04\n",
      "Epoch 138: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7566e-04 - val_loss: 1.3617e-04\n",
      "Epoch 139/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8384e-04\n",
      "Epoch 139: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.8054e-04 - val_loss: 1.0875e-04\n",
      "Epoch 140/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6862e-04\n",
      "Epoch 140: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7353e-04 - val_loss: 1.2570e-04\n",
      "Epoch 141/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7362e-04\n",
      "Epoch 141: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7389e-04 - val_loss: 1.0185e-04\n",
      "Epoch 142/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6634e-04\n",
      "Epoch 142: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7524e-04 - val_loss: 1.0152e-04\n",
      "Epoch 143/1000\n",
      "\u001b[1m147/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7178e-04\n",
      "Epoch 143: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7478e-04 - val_loss: 1.0029e-04\n",
      "Epoch 144/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7659e-04\n",
      "Epoch 144: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7582e-04 - val_loss: 1.0100e-04\n",
      "Epoch 145/1000\n",
      "\u001b[1m143/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7547e-04\n",
      "Epoch 145: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7569e-04 - val_loss: 1.0445e-04\n",
      "Epoch 146/1000\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6702e-04\n",
      "Epoch 146: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7419e-04 - val_loss: 1.0142e-04\n",
      "Epoch 147/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7681e-04\n",
      "Epoch 147: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7970e-04 - val_loss: 1.0617e-04\n",
      "Epoch 148/1000\n",
      "\u001b[1m145/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6789e-04\n",
      "Epoch 148: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7060e-04 - val_loss: 1.0613e-04\n",
      "Epoch 149/1000\n",
      "\u001b[1m148/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8722e-04\n",
      "Epoch 149: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7699e-04 - val_loss: 1.0058e-04\n",
      "Epoch 150/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8208e-04\n",
      "Epoch 150: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7427e-04 - val_loss: 1.0159e-04\n",
      "Epoch 151/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.6520e-04\n",
      "Epoch 151: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.7720e-04 - val_loss: 1.1643e-04\n",
      "Epoch 152/1000\n",
      "\u001b[1m144/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8286e-04\n",
      "Epoch 152: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7894e-04 - val_loss: 1.2945e-04\n",
      "Epoch 153/1000\n",
      "\u001b[1m150/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8298e-04\n",
      "Epoch 153: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7697e-04 - val_loss: 1.0137e-04\n",
      "Epoch 154/1000\n",
      "\u001b[1m151/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.7344e-04\n",
      "Epoch 154: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7608e-04 - val_loss: 1.1745e-04\n",
      "Epoch 155/1000\n",
      "\u001b[1m142/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8403e-04\n",
      "Epoch 155: val_loss did not improve from 0.00010\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7236e-04 - val_loss: 1.0385e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1000\n",
    "BATCH = 32\n",
    "\n",
    "# Callbacks comunes\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=80,             # se detiene si 80 epochs sin mejorar\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "def entrenar_modelo(nombre, build_fn, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Entrena un modelo (RNN, LSTM o GRU) con EarlyStopping y ModelCheckpoint\n",
    "    \"\"\"\n",
    "    print(f\"\\nEntrenando modelo {nombre}...\\n\")\n",
    "    model = build_fn()\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f\"mejor_{nombre.lower()}_ibex.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "# ==============================\n",
    "# ENTRENAR LOS TRES\n",
    "# ==============================\n",
    "\n",
    "rnn_model,  hist_rnn  = entrenar_modelo(\"RNN\",  build_rnn,  X_train, y_train, X_test, y_test)\n",
    "lstm_model, hist_lstm = entrenar_modelo(\"LSTM\", build_lstm, X_train, y_train, X_test, y_test)\n",
    "gru_model,  hist_gru  = entrenar_modelo(\"GRU\",  build_gru,  X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86519ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todas con la misma forma de entrada\n",
    "input_shape = (window_size, 1)\n",
    "\n",
    "def build_rnn():\n",
    "    model = Sequential([\n",
    "        SimpleRNN(64, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def build_gru():\n",
    "    model = Sequential([\n",
    "        GRU(64, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ba549",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8030688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# predicciones (en escala 0-1)\n",
    "y_pred_rnn  = rnn_model.predict(X_test)\n",
    "y_pred_lstm = lstm_model.predict(X_test)\n",
    "y_pred_gru  = gru_model.predict(X_test)\n",
    "\n",
    "# desescalar\n",
    "y_test_real = scaler_ibex.inverse_transform(y_test.reshape(-1, 1))\n",
    "pred_rnn_real  = scaler_ibex.inverse_transform(y_pred_rnn)\n",
    "pred_lstm_real = scaler_ibex.inverse_transform(y_pred_lstm)\n",
    "pred_gru_real  = scaler_ibex.inverse_transform(y_pred_gru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc9596",
   "metadata": {},
   "source": [
    "## Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7023b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN  -> MAE: 74.07773477687758 RMSE: 105.23687741646688\n",
      "LSTM -> MAE: 173.21227896733538 RMSE: 250.9443968193072\n",
      "GRU  -> MAE: 73.53813134966563 RMSE: 105.11021173693031\n"
     ]
    }
   ],
   "source": [
    "def evaluar(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return mae, rmse\n",
    "\n",
    "mae_rnn, rmse_rnn   = evaluar(y_test_real, pred_rnn_real)\n",
    "mae_lstm, rmse_lstm = evaluar(y_test_real, pred_lstm_real)\n",
    "mae_gru, rmse_gru   = evaluar(y_test_real, pred_gru_real)\n",
    "\n",
    "print(\"RNN  -> MAE:\", mae_rnn,  \"RMSE:\", rmse_rnn)\n",
    "print(\"LSTM -> MAE:\", mae_lstm, \"RMSE:\", rmse_lstm)\n",
    "print(\"GRU  -> MAE:\", mae_gru,  \"RMSE:\", rmse_gru)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df8c48d",
   "metadata": {},
   "source": [
    "# Graficar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0a738b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_test_real, label=\"Real\")\n",
    "plt.plot(pred_lstm_real, label=\"Pred LSTM\")\n",
    "plt.plot(pred_gru_real, label=\"Pred GRU\")\n",
    "plt.plot(pred_rnn_real, label=\"Pred RNN\")\n",
    "plt.title(\"IBEX - Real vs. Predicho (tramo test)\")\n",
    "plt.xlabel(\"Día\")\n",
    "plt.ylabel(\"Índice\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c06aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicción IBEX próximo día (LSTM): 10044.421\n"
     ]
    }
   ],
   "source": [
    "def predecir_siguiente(model, df_original, scaler, col_target=\"Price\", window_size=30):\n",
    "    # usamos el df limpio que tenías: ibex_df\n",
    "    ultimos = df_original[[col_target]].values.astype(float)[-window_size:]\n",
    "    ultimos_scaled = scaler.transform(ultimos)\n",
    "    X_input = ultimos_scaled.reshape((1, window_size, 1))\n",
    "    pred_scaled = model.predict(X_input)\n",
    "    pred = scaler.inverse_transform(pred_scaled)\n",
    "    return pred[0,0]\n",
    "\n",
    "pred_manana_lstm = predecir_siguiente(lstm_model, ibex_df, scaler_ibex, col_target=\"Price\", window_size=window_size)\n",
    "print(\"Predicción IBEX próximo día (LSTM):\", pred_manana_lstm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env11 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
